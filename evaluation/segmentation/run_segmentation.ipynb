{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for UNet training and testing.\n",
    "\n",
    "- If you  want to test synthetic data with our trained model, download `revised_model_epoch_29.pth` from OneDrive to `/evaluation/segmentation/` folder.  \n",
    "- If you want to train UNet on CholecSeg8k data, download the CholecSeg8k dataset from OneDrive to `evaulation/segmentation/cholecseg8k` folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10f9d2a8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d67fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install tqdm\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59cba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transforms as T\n",
    "import cv2\n",
    "import random\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image, ImageColor\n",
    "import io\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "DO_TRAINING = False\n",
    "FINAL_MODEL_PATH = \"revised_model_epoch_29.pth\"\n",
    "MODEL_NAME = \"fcn_resnet50\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "# Defining a color used to depict each semantic class being segmented\n",
    "META_DATA_ORIGINAL = [\n",
    "    (\"black_background\", (0,0,0)),\n",
    "    (\"abdominal_wall\", (33, 191, 197)),\n",
    "    (\"liver\", (231, 126, 9)),\n",
    "    (\"gastrointestinal_tract\", (209, 53, 84)),\n",
    "    (\"fat\", (80, 155, 4)),\n",
    "    (\"grasper\", (255, 207, 210)),\n",
    "    (\"connective_tissue\", (169, 52, 199)),\n",
    "    (\"blood\", (229, 18, 18)),\n",
    "    (\"cystic_duct\", (149, 50, 18)),\n",
    "    (\"l-hook_electrocautery\", (46, 43, 180)),\n",
    "    (\"gallbladder\", (148, 55, 66)),\n",
    "    (\"hepatic_vein\", (214, 51, 149)),\n",
    "    (\"liver_ligament\", (240, 79, 10)),\n",
    "]\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "# THESE 2 LINES REDUCE / MERGE CLASSES. TO REDUCE PUT 255 VALUE. TO MERGE PUT DESIRED CLASS AS VALUE. REMAINING CLASSES HAVE TO BE FROM 0 to N.\n",
    "\n",
    "CLASSES_TO_IGNORE = [\"black_background\",\"gastrointestinal_tract\", \"connective_tissue\", \"blood\", \"cystic_duct\", \"l-hook_electrocautery\",\"hepatic_vein\", \"liver_ligament\"]\n",
    "REPLACE_CLASS = {0:255, 1:0, 2:1,3:255,4:2, 5:3,6:255,7:255, 8:255, 9:3, 10:4,11:255, 12:255, 13:3, 14:3, 15:3, 16:3}\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "META_DATA = [x for x in META_DATA_ORIGINAL if x[0] not in CLASSES_TO_IGNORE]\n",
    "\n",
    "\n",
    "# Optimizer parameters\n",
    "learning_rate = 0.00125\n",
    "momentum = 0.9\n",
    "power = 0.9\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eab8ca5",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c9c5420",
   "metadata": {},
   "source": [
    "Defining some reusable function that we will use throughout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d81b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_list(images, fill_value=0):\n",
    "    max_size = tuple(max(s) for s in zip(*[img.shape for img in images]))\n",
    "    batch_shape = (len(images),) + max_size\n",
    "    batched_imgs = images[0].new(*batch_shape).fill_(fill_value)\n",
    "    for img, pad_img in zip(images, batched_imgs):\n",
    "        pad_img[..., : img.shape[-2], : img.shape[-1]].copy_(img)\n",
    "    return batched_imgs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = list(zip(*batch))\n",
    "    batched_imgs = cat_list(images, fill_value=0)\n",
    "    batched_targets = cat_list(targets, fill_value=255)\n",
    "    return batched_imgs, batched_targets\n",
    "\n",
    "# Helper function to do a cross entropy loss between the ground truth and predicted values\n",
    "def criterion(inputs, target):\n",
    "    losses = {}\n",
    "    for name, x in inputs.items():\n",
    "        losses[name] = nn.functional.cross_entropy(x, target, ignore_index=255)\n",
    "    if len(losses) == 1:\n",
    "        return losses[\"out\"]\n",
    "    return losses[\"out\"] + 0.5 * losses[\"aux\"]\n",
    "\n",
    "\n",
    "# Helper function to compute relevant metrics using a confusion matrix\n",
    "# see: https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "class ConfusionMatrix:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.mat = None\n",
    "\n",
    "    def update(self, a, b):\n",
    "        n = self.num_classes\n",
    "        if self.mat is None:\n",
    "            self.mat = torch.zeros((n, n), dtype=torch.int64, device=a.device)\n",
    "        with torch.no_grad():\n",
    "            k = (a >= 0) & (a < n)\n",
    "            inds = n * a[k].to(torch.int64) + b[k]\n",
    "            self.mat += torch.bincount(inds, minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "    def reset(self):\n",
    "        self.mat.zero_()\n",
    "\n",
    "    def compute(self):\n",
    "        h = self.mat.float()\n",
    "        acc_global = torch.diag(h).sum() / h.sum()\n",
    "        acc = torch.diag(h) / h.sum(1)\n",
    "        iou = torch.diag(h) / (h.sum(1) + h.sum(0) - torch.diag(h))\n",
    "        return acc_global, acc, iou\n",
    "    \n",
    "    # Return overall accuracy, per-class accuracy, per-class Intersection over Union (IoU) and mean IoU\n",
    "    def __str__(self):\n",
    "        acc_global, acc, iou = self.compute()\n",
    "        return (\"global correct: {:.2f}\\naverage row correct: {}\\nIoU: {}\\nmean IoU: {:.2f}\").format(\n",
    "            acc_global.item() * 100,\n",
    "            [f\"{i:.1f}\" for i in (acc * 100).tolist()],\n",
    "            [f\"{i:.1f}\" for i in (iou * 100).tolist()],\n",
    "            iou.mean().item() * 100,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a7c96b6",
   "metadata": {},
   "source": [
    "## CholeSeg8k\n",
    "The CholecSeg8k dataset [1] consists of subset of Cholec80 [2] annotated with semantic segmentation labels with 13 semantic classes for 17 video clips.\n",
    "\n",
    "\n",
    "1. _Hong, W-Y., C-L. Kao, Y-H. Kuo, J-R. Wang, W-L. Chang, and C-S. Shih. \"CholecSeg8k: A Semantic Segmentation Dataset for Laparoscopic Cholecystectomy Based on Cholec80.\" arXiv preprint arXiv:2012.12453 (2020)._\n",
    "\n",
    "2. _Twinanda, Andru P., Sherif Shehata, Didier Mutter, Jacques Marescaux, Michel De Mathelin, and Nicolas Padoy. \"Endonet: a deep architecture for recognition tasks on laparoscopic videos.\" IEEE transactions on medical imaging 36, no. 1 (2016): 86-97._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0af3530c",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0e8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a dataset class that delivers images and correponding ground truth segmentation masks\n",
    "# from the CholecSeg8k. Please refer to Lecture 6 for more info on torch Datasets.\n",
    "def map_values(x):\n",
    "    return REPLACE_CLASS.get(x, x)\n",
    "\n",
    "class CholecDatasetSegm(torch.utils.data.Dataset):\n",
    "    def __init__(self, gt_json, meta_data, root_dir = \"./cholecseg8k\", data_split = \"train\", transforms = None):\n",
    "        self.gt_json = gt_json\n",
    "        self.root_dir = root_dir\n",
    "        self.data_split = data_split\n",
    "        self.transforms = transforms\n",
    "        gt_data = json.load(open(gt_json))\n",
    "        self.images = [os.path.join(self.root_dir, g[\"file_name\"]) for g in gt_data]\n",
    "        self.targets = [os.path.join(self.root_dir, g[\"mask_name\"]) for g in gt_data]\n",
    "        self.metadata = meta_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
    "        target = Image.open(self.targets[index]).convert(\"L\")\n",
    "        target = target.resize(img.size, resample=Image.NEAREST)\n",
    "        \n",
    "        if CLASSES_TO_IGNORE:\n",
    "            target = np.array(target)\n",
    "            \n",
    "            # use numpy's vectorize function to apply the mapping to the whole array\n",
    "            map_func = np.vectorize(map_values)\n",
    "            target = map_func(target)\n",
    "\n",
    "            target = Image.fromarray(target.astype(np.uint8), mode='L')\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e7df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationPresetTrain:\n",
    "    def __init__(self, base_size, crop_size, hflip_prob=0.5, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        min_size = int(0.5 * base_size)\n",
    "        max_size = int(2.0 * base_size)\n",
    "\n",
    "        trans = [T.RandomResize(min_size, max_size)]\n",
    "        if hflip_prob > 0:\n",
    "            trans.append(T.RandomHorizontalFlip(hflip_prob))\n",
    "        trans.extend(\n",
    "            [\n",
    "                T.RandomCrop(crop_size),\n",
    "                T.PILToTensor(),\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        )\n",
    "        self.transforms = T.Compose(trans)\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        return self.transforms(img, target)\n",
    "\n",
    "\n",
    "class SegmentationPresetEval:\n",
    "    def __init__(self, base_size, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        self.transforms = T.Compose(\n",
    "            [\n",
    "                T.RandomResize(base_size, base_size),\n",
    "                T.PILToTensor(),\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        return self.transforms(img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0fd7802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Data Loaders for the training and testing splits.\n",
    "# Please refer to Lecture 6 for more info on torch Data Loaders.\n",
    "\n",
    "\n",
    "def get_transform(train=True):\n",
    "    if train:\n",
    "        return SegmentationPresetTrain(base_size=512, crop_size=400)\n",
    "    else:\n",
    "        return SegmentationPresetEval(base_size=400)\n",
    "    \n",
    "# Train loader\n",
    "dataset = CholecDatasetSegm(\"./cholecseg8k/train_final.json\", META_DATA, data_split=\"train\", transforms=get_transform())\n",
    "num_classes = len(META_DATA)\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# Test loader\n",
    "dataset_test = CholecDatasetSegm(\"./cholecseg8k/val_final.json\", META_DATA, data_split=\"val\", transforms=get_transform(False))\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, sampler=test_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60e8373e",
   "metadata": {},
   "source": [
    "## CREATE IRCAD TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd86f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mIOU for mixed style\n",
    "\n",
    "data_path_25_66 = '/path/to/data' #output_vid25_66\n",
    "data_path_01_49 = '/path/to/data' #output_vid01_49\n",
    "data_path_52_56 = '/path/to/data' #output_vid52_56\n",
    "\n",
    "data_path_seg = '/path/to/segmentation_maps' # Segmentation maps need to have CholecSeg classes. They were generated in the preprocessing step\n",
    "\n",
    "# get list of files\n",
    "rand_dict = {1: data_path_25_66,\n",
    "            2: data_path_01_49,\n",
    "            3: data_path_52_56}\n",
    "\n",
    "filenames_common =set(os.listdir(rand_dict[1]))& set(os.listdir(rand_dict[2])) & set(os.listdir(rand_dict[3]))\n",
    "filenames = [os.path.join(rand_dict[random.randint(1, 1)], f) for f in filenames_common]\n",
    "\n",
    "random.seed(420420)\n",
    "filenames = random.sample(filenames,10)\n",
    "\n",
    "# get list of corresponding segmentation masks\n",
    "masks = [os.path.join(data_path_seg, os.path.basename(f)) for f in filenames]\n",
    "\n",
    "# Save json with test files + masks\n",
    "data = []\n",
    "for f, m in zip(filenames, masks):\n",
    "    data.append({'file_name': f, 'mask_name': m})\n",
    "with open('test_synthetic.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "#Test IRCAD\n",
    "transforms_ircad = SegmentationPresetEval(base_size=512)\n",
    "dataset_test_ircad = CholecDatasetSegm(\"test_synthetic.json\", META_DATA, root_dir='',data_split=\"val\", transforms=transforms_ircad)\n",
    "test_sampler_ircad = torch.utils.data.SequentialSampler(dataset_test_ircad)\n",
    "data_loader_test_ircad = torch.utils.data.DataLoader(dataset_test_ircad, batch_size=1, sampler=test_sampler_ircad, collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "773fc52d",
   "metadata": {},
   "source": [
    "## Segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "234be465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jk/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jk/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=FCN_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.segmentation.__dict__[MODEL_NAME](pretrained=True)\n",
    "model.classifier[4] = nn.Conv2d(512, num_classes, 1)\n",
    "model.aux_classifier [4] = nn.Conv2d(256, num_classes, 1)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88470175",
   "metadata": {},
   "source": [
    "## Optimizer and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3a6ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_optimize = [\n",
    "    {\"params\": [p for p in model.backbone.parameters() if p.requires_grad]},\n",
    "    {\"params\": [p for p in model.classifier.parameters() if p.requires_grad]},\n",
    "]\n",
    "params = [p for p in model.aux_classifier.parameters() if p.requires_grad]\n",
    "params_to_optimize.append({\"params\": params, \"lr\": learning_rate * 10})\n",
    "\n",
    "iters_per_epoch = len(data_loader)\n",
    "optimizer = torch.optim.SGD(params_to_optimize, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda x: (1 - x / (iters_per_epoch * NUM_EPOCHS)) ** power)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c52c47d6",
   "metadata": {},
   "source": [
    "## Helper function for training and validation for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91e82f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train\n",
    "def train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, device):\n",
    "    model.train()\n",
    "    train_loss  = 0.0\n",
    "    pbar = tqdm(data_loader)\n",
    "    for image, target in pbar:\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_description(\"train_loss: {:.3f} lr: {:.3f}\".format(loss.item(), \n",
    "                                                                    optimizer.param_groups[0][\"lr\"]))\n",
    "    train_loss /= len(data_loader)\n",
    "    return train_loss, optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "# Helper function to evaluate\n",
    "def evaluate(model, data_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    confmat = ConfusionMatrix(num_classes)\n",
    "    pbar = tqdm(data_loader)\n",
    "    with torch.no_grad():\n",
    "        for image, target in pbar:\n",
    "            image, target = image.to(device), target.to(device)\n",
    "            output = model(image)\n",
    "            output = output[\"out\"]\n",
    "            confmat.update(target.flatten(), output.argmax(1).flatten())\n",
    "            pbar.set_description(\"eval\")\n",
    "    return confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19d306f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded model weights from revised_model_epoch_29.pth \n",
      "missing keys = []  invalid keys []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb3d4bbbd104087a019c397b6abb798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_global: 82.877 iou: 65.003\n",
      "confmat: global correct: 82.88\n",
      "average row correct: ['84.9', '74.8', '97.3', '86.2', '90.7']\n",
      "IoU: ['61.9', '72.6', '87.9', '63.8', '38.8']\n",
      "mean IoU: 65.00\n"
     ]
    }
   ],
   "source": [
    "if DO_TRAINING:\n",
    "    pbar = tqdm(range(NUM_EPOCHS))\n",
    "    # Train and evaluate after each epoch\n",
    "    for epoch in pbar:\n",
    "        train_loss, last_lr = train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, DEVICE)\n",
    "        confmat = evaluate(model, data_loader_test, device=DEVICE, num_classes=num_classes)\n",
    "        acc_global, acc, iu = confmat.compute()    \n",
    "        pbar.set_description(\n",
    "            \"train_loss: {:.3f} last_lr: {:.3f} acc_global: {:.3f} iou: {:.3f}\".format(\n",
    "                train_loss, last_lr, acc_global.item() * 100, iu.mean().item() * 100\n",
    "            )\n",
    "        )\n",
    "        print(\"confmat:\", confmat)\n",
    "        torch.save(model.state_dict(), \"revised_model_epoch_\"+str(epoch)+\".pth\")\n",
    "else:\n",
    "    m,v = model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=DEVICE))\n",
    "    print(\"=> loaded model weights from {} \\nmissing keys = {}  invalid keys {}\".format(FINAL_MODEL_PATH, m, v))\n",
    "\n",
    "\n",
    "    confmat = evaluate(model, data_loader_test_ircad, device=DEVICE, num_classes=num_classes)\n",
    "    acc_global, acc, iu = confmat.compute()    \n",
    "    print(\n",
    "        \"acc_global: {:.3f} iou: {:.3f}\".format(\n",
    "            acc_global.item() * 100, iu.mean().item() * 100\n",
    "        )\n",
    "    )\n",
    "    print(\"confmat:\", confmat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
